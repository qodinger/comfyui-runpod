# RunPod Setup — Persistent Model Storage and Deployment

This document explains how to deploy the repo on RunPod so the large Hugging Face model is downloaded to a persistent volume and reused across runs. This is the recommended approach to avoid filling ephemeral `/app` and to prevent repeated multi-GB downloads.

## Summary (quick)
- Create a RunPod network volume (recommended >= 100 GB) and mount it at `/runpod-volume`.
- Configure RunPod env vars: `HF_TOKEN` (secret), `HF_MODEL_ID`, `HF_MODEL_FILE`.
- Use the existing startup command: `/bin/bash start_serverless.sh`.
- The startup script will download the model into `/runpod-volume/models/checkpoints` and symlink `/app/models` and RunPod state files to the persistent volume.

## Create and mount a volume
1. In RunPod > Storage, create a network volume (name it e.g. `comfyui-models`) sized 100 GB or larger.
2. In your serverless pod settings, add a mount: Host path `/runpod-volume` -> container path `/runpod-volume` (or configure the RunPod UI equivalent).

## Environment variables (set in RunPod UI)
- `HF_TOKEN` (Secret) — your Hugging Face token (required for gated/private models).
- `HF_MODEL_ID` — example: `tyecode/AnythingXL` (owner/repo).
- `HF_MODEL_FILE` — example: `AnythingXL_xl.safetensors` (filename in repo).

If the model is public you can omit `HF_TOKEN` but it's recommended for reliability.

## Startup command
Set the container startup/command to:
```
/bin/bash start_serverless.sh
```

The script will:
- Prefer persistent paths (`/runpod-volume` then `/workspace`) and create `/runpod-volume/models/checkpoints`.
- Symlink `/app/models` -> `/runpod-volume/models` so ComfyUI finds models normally.
- Symlink `/app/.runpod_jobs.pkl` and its `.lock` to the persistent prefix to avoid "No space left on device" errors when `/app` is ephemeral.
- Download the specified Hugging Face file into `/runpod-volume/models/checkpoints`.

## Manual download (if you prefer to pre-populate the volume)
If you want to download the model yourself into the mounted volume (e.g., from your CI or locally), run the included downloader script inside a container that has access to the mounted volume.

Example (inside container or local dev with volume mounted):
```
# put your token in env (do not commit)
HF_TOKEN="hf_xxx..."
python3 scripts/download_hf_model.py --model-id=tyecode/AnythingXL --filename=AnythingXL_xl.safetensors --dest=/runpod-volume/models/checkpoints
```

The script will place the file at `/runpod-volume/models/checkpoints/AnythingXL_xl.safetensors` and the startup script will detect it and skip re-downloading.

## Quick verification commands (once pod is running)
```
# check free space on persistent volume
df -h /runpod-volume
# list model file
ls -lh /runpod-volume/models/checkpoints
# check symlinks
ls -l /app/.runpod_jobs.pkl /app/.runpod_jobs.pkl.lock /app/models
# tail logs
# In RunPod UI: view worker logs and look for "✅ Model downloaded successfully" and "✅ ComfyUI is ready!"
```

## Troubleshooting
- If you still see "No space left on device" in logs, confirm the volume is mounted at `/runpod-volume` and `df -h /runpod-volume` shows plenty of space.
- If model download fails with network errors, verify `HF_TOKEN` and network access; try the manual downloader command above.
- If ComfyUI times out or shows import errors referencing NumPy/RMSNorm/pytree, you may need to adjust Python package versions (see `DEPLOYMENT_GUIDE.md` or ask me to pin `numpy<2` or upgrade `torch`).

## Alternatives
- Bake the model into the Docker image during build (increases image size by ~7GB and makes builds slower). Use Docker BuildKit secrets if you must include an `HF_TOKEN` at build-time.
- Host the model in an object store (S3/GCS) and download from there during startup into `/runpod-volume`.

If you want, I can add a small GitHub Actions workflow to pre-populate a volume or to pre-build an image with the model using BuildKit secrets — tell me which and I will add it.

---
Generated by the repo maintainer helper: follow above steps and redeploy.
